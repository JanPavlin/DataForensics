{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Jupyter notebook we filter the image data, select the models and images and download thi data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "source: https://medium.com/@adrianmrit/creating-simple-image-datasets-with-flickr-api-2f19c164d82f\n",
    "'''\n",
    "from flickrapi import FlickrAPI\n",
    "import os\n",
    "import time\n",
    "from progress.bar import Bar\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "KEY = 'YourFlickrAPIKey'\n",
    "SECRET = 'Mellon'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from our main files\n",
    "# Link: https://faui1-gitlab.cs.fau.de/mullanptr/flickr_data\n",
    "# files are merged on 'imgID'\n",
    "df3 = pd.merge(\n",
    "    pd.read_csv('initial_data.csv'),\n",
    "    pd.read_csv('parsed_data.csv'),\n",
    "    on='imgID',\n",
    "    how='outer')\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most common makes of images \n",
    "# set number of best models\n",
    "num_best = 9\n",
    "common_make = list(df3[\"Make_normalized\"].value_counts()[:num_best].index)\n",
    "print(common_make)\n",
    "\n",
    "# Filter the dataset so we have only common makes makes \n",
    "df_filtered = df3.loc[df3['Make_normalized'].isin(common_make)]\n",
    "df_filtered.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques = df_filtered['unique_id'].unique()\n",
    "serija = df_filtered[\"unique_id\"].value_counts().describe()\n",
    "#df_cut = \n",
    "#\n",
    "counts = df_filtered['unique_id'].value_counts().rename('unique_counts')\n",
    "print(counts)\n",
    "df_filtered1 = df_filtered.merge(counts.to_frame(),\n",
    "                                left_on='unique_id',\n",
    "                                right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wil check for the occurences of each unique combination, \n",
    "# for start lets just get unique combinations\n",
    "makes = pd.unique(df_filtered[[\"Make_normalized\"]].values.ravel())\n",
    "df_filtered[\"unique_id\"] = -1\n",
    "c = 0\n",
    "\n",
    "for make in makes: \n",
    "    sub_df = df_filtered.loc[df_filtered[\"Make_normalized\"] == make]\n",
    "    models = pd.unique(sub_df[[\"Model_normalized\"]].values.ravel())\n",
    "    print(make, sub_df.shape, len(models))\n",
    "    for model in models: \n",
    "        #print(\"     -\", model)\n",
    "        df_filtered.loc[(df_filtered['Make_normalized'] == make) &\n",
    "                        (df_filtered['Model_normalized'] == model), \n",
    "                        'unique_id'] = c\n",
    "        c+=1        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the uniques\n",
    "uniques = df_filtered['unique_id'].unique()\n",
    "# I think this is useless\n",
    "serija = df_filtered[\"unique_id\"].value_counts().describe()\n",
    "\n",
    "# Count the number of unique occurences\n",
    "counts = df_filtered['unique_id'].value_counts().rename('unique_counts')\n",
    "print(counts)\n",
    "# Megre with df \n",
    "df_filtered1 = df_filtered.merge(counts.to_frame(),\n",
    "                                left_on='unique_id',\n",
    "                                right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating reserve df to have backup if anything is deleted\n",
    "df_reserve = df_filtered\n",
    "df_filtered = df_filtered1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets select the combinations of camera makes and models that has more than 200 images \n",
    "df_filtered = df_filtered[df_filtered.unique_counts > 200]\n",
    "#cdf.info()\n",
    "#cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Generate empty dataframe that we will use for download \n",
    "df_download = df_filtered.loc[df_filtered['Make_normalized']=='Generate_empty']\n",
    "\n",
    "#sample random n_sample samples. \n",
    "\n",
    "# ** NOTE We have set 9 common makes \n",
    "# If number of downloaded images will be then 9* n_sample* n_models\n",
    "# \n",
    "n_sample = 200\n",
    "n_models = 20\n",
    "i = 0\n",
    "\n",
    "for make in common_make:  \n",
    "    print(make)\n",
    "    sub_df = df_filtered.loc[df_filtered[\"Make_normalized\"] == make]\n",
    "    models = pd.unique(sub_df[[\"Model_normalized\"]].values.ravel())\n",
    "    \n",
    "    #print(type(models))\n",
    "    #print(rmodels)\n",
    "    \n",
    "    j = 0\n",
    "    while j < n_models:\n",
    "        # Select random model \n",
    "        model = np.random.choice(models, 1)[0]\n",
    "        #print(model)\n",
    "        dfh = df_filtered[(df_filtered['Make_normalized']==make)&\n",
    "                          (df_filtered['Model_normalized'] == model)]\n",
    "        #print(dfh.shape)\n",
    "        \n",
    "        # This is a check that should never be True. \n",
    "        if dfh.shape[0] < n_sample: \n",
    "            continue\n",
    "        j+=1\n",
    "        \n",
    "        # Sample n images \n",
    "        df_subset = df_filtered[(df_filtered['Make_normalized']==make)&\n",
    "                                (df_filtered['Model_normalized'] == model)].sample(n=n_sample)\n",
    "        df_subset['type'] = i\n",
    "        df_download = pd.concat([df_download, df_subset])\n",
    "print(df_download.info())\n",
    "    \n",
    "# We get the dataframe with url's we will download\n",
    "#df_download.head(20),,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images and save data\n",
    "\n",
    "# 1. download image, save EXIF\n",
    "data_dict = dict()\n",
    "count = 0\n",
    "for index, row in df_download.iterrows():\n",
    "    if (count+1)%100 == 0: \n",
    "        print(count+1)\n",
    "    url = row['url_o']\n",
    "    response=requests.get(url,stream=True) \n",
    "    \n",
    "    # Download image, get EXIF DATA\n",
    "    try: \n",
    "        image = Image.open(requests.get(url, stream=True).raw)\n",
    "        exifdata = image.getexif()\n",
    "        \n",
    "        #Save EXIF data in dict <-- this dictionary is the goal of Downloader \n",
    "        data_dict[url] = exifdata\n",
    "       \n",
    "            \n",
    "    except Exception as e: \n",
    "        #print(\"Problem - \" , url, \": \",  e)\n",
    "        try: \n",
    "            df_download = df_download.drop(index)\n",
    "        except: \n",
    "            pass\n",
    "    count +=1\n",
    "\n",
    "# 2. Save 'data_dict' with exif info and 'download_df' with normalized data info\n",
    "with open('filtered9b.pickle', 'wb') as handle:\n",
    "    pickle.dump(data_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('download_meta9b.pickle', 'wb') as handle:\n",
    "    pickle.dump(df_download, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_download.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
