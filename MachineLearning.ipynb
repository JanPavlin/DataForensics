{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved data pickle: \n",
    "with open('filtered9b.pickle', 'rb') as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "    \n",
    "print(len(data_dict))\n",
    "\n",
    "with open('download_meta9b.pickle', 'rb') as handle:\n",
    "    df_download = pickle.load(handle)\n",
    "    \n",
    "print(len(data_dict))\n",
    "#print(data_dict)\n",
    "df_download.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE in downloader\n",
    "column_values = df_download[[\"Make_normalized\", \"Model_normalized\"]].values.ravel()\n",
    "unique_values =  pd.unique(column_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done in downloader\n",
    "makes = pd.unique(df_download[[\"Make_normalized\"]].values.ravel())\n",
    "print(makes)\n",
    "df_download[\"unique_id\"] = -1\n",
    "c = 0\n",
    "for make in makes: \n",
    "    sub_df = df_download[df_download[\"Make_normalized\"] == make]\n",
    "    print(make)\n",
    "    models = pd.unique(sub_df[[\"Model_normalized\"]].values.ravel())\n",
    "    for model in models: \n",
    "        print(\"     -\", model)\n",
    "        #test_df = df_download[(df_download[\"Make_normalized\"] == make and df_download[\"Model_normalized\"] == model)]\n",
    "        #df_download[\"unique_id\"] = np.where(((df_download[\"Make_normalized\"] == make) & (df_download[\"Model_normalized\"] == model)),c)\n",
    "        #, 'unique_id'] = c\n",
    "        df_download.loc[(df_download['Make_normalized'] == make) & (df_download['Model_normalized'] == model), 'unique_id'] = c\n",
    "        c+=1        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think this is just some test i forgot to delete \n",
    "print(df_download[\"unique_id\"].value_counts())\n",
    "random_tag = random.choice(list(df_download['unique_id'].unique()))\n",
    "\n",
    "train = df_download[df_download['unique_id']!=random_tag]\n",
    "test = df_download[df_download['unique_id'] == random_tag]\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now this is an important part \n",
    "# Basicly we iterate over the dictionary and construct a pandas dataframe \n",
    "\n",
    "all_tags = dict()\n",
    "mean_tags = list()\n",
    "models = dict()\n",
    "c = 0\n",
    "\n",
    "df = pd.DataFrame([], dtype=int)\n",
    "\n",
    "st = time.time()\n",
    "for key in data_dict: \n",
    "    # Key is the url link of the image that we use for indexing\n",
    "    \n",
    "    # Create index\n",
    "    df = df.append(pd.Series(name=key))\n",
    "    c_tags = 0\n",
    "    \n",
    "    # Lets iterate over tags that are giver for the image \n",
    "    for tag_id in data_dict[key]:\n",
    "        c_tags += 1\n",
    "        \n",
    "        if tag_id not in all_tags: \n",
    "            all_tags[tag_id] = 1\n",
    "        all_tags[tag_id] += 1\n",
    "        \n",
    "        # Here we get the tag name (which we dont need/use)\n",
    "        tag = TAGS.get(tag_id, tag_id)\n",
    "        \n",
    "        data = data_dict[key].get(tag_id)\n",
    "        if isinstance(data, bytes):\n",
    "            try: \n",
    "                data = data.decode('utf-8', 'ignore').encode(\"utf-8\")\n",
    "            except: \n",
    "                pass\n",
    "        try: \n",
    "            \n",
    "            if str(tag_id) in df.columns: \n",
    "                # if tag_ id is already in columns - we just insert the values \n",
    "                df.loc[key, str(tag_id)] = data\n",
    "            else: \n",
    "                # Else we have to add another column with tag_id \n",
    "                df[str(tag_id)] = np.nan   \n",
    "                df.loc[key, str(tag_id)] = str(data)\n",
    "\n",
    "            \n",
    "        except Exception as e: \n",
    "            pass\n",
    "            #print(data)\n",
    "            #print(e)\n",
    "            #print(f\"{tag:25}: {data}\")\n",
    "     \n",
    "    mean_tags.append(c_tags)\n",
    "    \n",
    "et = time.time()\n",
    "\n",
    "print(\"Median number of tags per imamge:\", np.mean(np.array(mean_tags)))\n",
    "print(\"Consumed time:\", et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for nerds\n",
    "df.info()\n",
    "# Also get percentage of missing values. \n",
    "per_missing = (df.isnull().sum()/len(df)).mean()\n",
    "print(\"Percentage of missing values is:\", per_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this we create similar dataset that has same shape, but has 1 instead of value and 0 instead of None\n",
    "df2 = df.notnull().astype('int')\n",
    "df2.info()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    df2 = df2.drop(list(one_hot), axis=1)\n",
    "    \n",
    "except:\n",
    "    pass\n",
    "df2.index.names = ['url_o']\n",
    "df2 = df2.join(dftest)\n",
    "#df2.merge(dftest, how='left')\n",
    "#df2.index.names = ['url_o']\n",
    "df2.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(df_download[\"type\"].value_counts())\n",
    "\n",
    "def generate_random():\n",
    "    \n",
    "    random_tag = random.choice(list(df_download['unique_id'].unique()))\n",
    "    print(\"generating random:\", random_tag)\n",
    "    return random_tag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(traindata_X, traindata_Y):\n",
    "    # Import the model we are using\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # Instantiate model with decision trees\n",
    "    rf = RandomForestRegressor(random_state = 42)\n",
    "    # Train the model on training data\n",
    "    rf.fit(traindata_X, traindata_Y);\n",
    "    return rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solution(testdata_Y):\n",
    "    \n",
    "    testdata_Y.head()\n",
    "    solution = -1\n",
    "    row = testdata_Y.iloc[0] # first row of data frame (Aleshia Tomkiewicz)\n",
    "    for i in range(len(targets)): \n",
    "        target = targets[i]\n",
    "        if testdata_Y[target].any() == 1:\n",
    "            solution = i\n",
    "    #print(\"Getting the solution:\", solution)\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "def predict(rf, testdata_X):\n",
    "    predictions = np.rint(rf.predict(testdata_X))\n",
    "    res = []\n",
    "    for row in predictions: \n",
    "        res.append(np.where(row == np.amax(row))[0][0])\n",
    "    #print(\"Predicting:\", res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(traindata_X.columns)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this part the main machine leadning happens \n",
    "\n",
    "# Set number of iterations: \n",
    "num_iter = 50\n",
    "tr = [0,0]\n",
    "all_sol = []\n",
    "targets = ['Apple','Canon','Fujifilm','Kodak','Nikon','Olympus','Panasonic','Samsung','Sony']\n",
    "features = df2.columns.difference(targets)\n",
    "all_res = []\n",
    "\n",
    "# We repeat learning for given number of iterations \n",
    "for i in range(num_iter):\n",
    "    tag = generate_random()\n",
    "    # Split training and test data based on random tag\n",
    "    train = df2[df2['unique_id']!=tag]\n",
    "    test = df2[df2['unique_id'] == tag]\n",
    "    \n",
    "    \n",
    "    traindata = train\n",
    "    testdata = test\n",
    "    \n",
    "    #Drop rows that have information about models\n",
    "    try : \n",
    "        traindata = traindata.drop(['unique_id'], axis=1)\n",
    "    except: \n",
    "        pass\n",
    "    try : \n",
    "        testdata = testdata.drop(['unique_id'], axis=1)\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    # Split features and targets\n",
    "    traindata_X = traindata[traindata.columns.difference(targets)]\n",
    "    traindata_Y = traindata[targets]\n",
    "    \n",
    "    testdata_X = testdata[testdata.columns.difference(targets)]\n",
    "    testdata_Y = testdata[targets]\n",
    "    \n",
    "    solution = get_solution(testdata_Y)\n",
    "    \n",
    "    all_sol.append(solution)\n",
    "    # Here the Random forrest classifier will learn \n",
    "    rf = learn(traindata_X, traindata_Y)\n",
    "    # And predict\n",
    "    res = predict(rf, testdata_X)\n",
    "    all_res.append(res)\n",
    "    \n",
    "    # Quick analysis\n",
    "    for el in res: \n",
    "        if el == solution: \n",
    "            tr[0]+=1\n",
    "        else: \n",
    "            tr[1]+=1\n",
    "    print(tr)\n",
    "print(\"FINISHED\")\n",
    "print(\"[CORRECT: FALIURES]\")\n",
    "print(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "#1\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "start_time = time.time()\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "import pandas as pd\n",
    "forest_importances = pd.Series(importances, index=features)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get feature importances\n",
    "#2\n",
    "best_attrs = []\n",
    "from PIL.ExifTags import TAGS\n",
    "#plt.barh(features, rf.feature_importances_)\n",
    "best = rf.feature_importances_.argsort()[-10:][::-1]\n",
    "for el in best: \n",
    "    tag = TAGS.get(int(features[el]))\n",
    "    best_attrs.append(features[el])\n",
    "    print(features[el], \"(\", tag, \"):\", rf.feature_importances_[el])\n",
    "print(\"_____________\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT and print missing values based on feature importances \n",
    "\n",
    "targets = ['Apple','Canon','Fujifilm','Kodak','Nikon','Olympus','Panasonic','Samsung','Sony']\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for col in best_attrs[:4]:\n",
    "    fig = plt.figure(figsize=(8, 4))\n",
    "    \n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    ax.title.set_text(\"Tag_ID: \"+ col +\" - \" + TAGS.get(int(col)))\n",
    "    res = []\n",
    "    for target in targets: \n",
    "        sub_df = df2.loc[df2[target] == 1]\n",
    "        #print(sub_df.info())\n",
    "        # Calculate percentage \n",
    "        sol = sub_df[col].astype(bool).sum(axis=0)*100/len(sub_df)\n",
    "        print(target , \",\", \"target\", sol)\n",
    "        res.append(sol)\n",
    "        \n",
    "    \n",
    "    ax.bar(targets,res)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT RESULTS? I dont remember. \n",
    "# it is inportant for the future\n",
    "res_together = []\n",
    "sol_together = []\n",
    "for i in range(len(all_res)): \n",
    "    tmp_res = all_res[i]\n",
    "    tmp_sol = all_sol[i]\n",
    "    for j in range(len(tmp_res)):\n",
    "        res_together.append(tmp_res[j])\n",
    "        sol_together.append(tmp_sol)\n",
    "\n",
    "#print(res_together)\n",
    "#print()\n",
    "#print(sol_together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "#plot_confusion_matrix(res_together, sol_together)  \n",
    "cm = confusion_matrix(sol_together, res_together)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matriix with colors :) \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "ls = [0, 1] # your y labels\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save the results\n",
    "np.save(\"y_test.npy\", np.array(sol_together))\n",
    "np.save(\"pred.npy\", np.array(res_together))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another confusion matrix that was used in report has been done using: \n",
    "https://github.com/wcipriano/pretty-print-confusion-matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
